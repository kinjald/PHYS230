{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2SbQ9mHqUx-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Function Approximation with PyTorch\n",
    "\n",
    "The objective of this tutorial is learn the basics of PyTorch.\n",
    "\n",
    "To this end, we cosider the task of learning a function, with two simple examples:\n",
    "\n",
    "Polynomial class (considered before in class):\n",
    "$$f(x) =2x-10x^5+15x^{10}, \\quad x\\in(0, 1)$$\n",
    "\n",
    "Damped oscillations (example for PINN):\n",
    "$$ f(x) = e^{-x/\\delta} \\sin(\\omega x), \\quad x\\in(0, 2 \\pi) $$\n",
    "\n",
    "by using feed-forward dense neural networks.\n",
    "\n",
    "Three Steps:\n",
    "1.  Dataset generation\n",
    "2.  Build the Pytorch Model\n",
    "3.  Pytorch training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNmKayODqUyA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# $\\mathbf{\\text{Step 1: Dataset Generation}}$\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The training data is generated by drawing samples from the equation\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "where $f(x_i)$ is some fixed function, and $\\eta_i$ is a Gaussian, uncorrelated noise variable such that\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \n",
    "$$\n",
    "$$\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "We will refer to the $f(x_i)$ as the **true model** used to generate the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "111sRdoleTML"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUQdfjjVew1P"
   },
   "outputs": [],
   "source": [
    "# Uncomment and complete the following code to generate synthetic sine wave data with added noise\n",
    "def generate_data(num_points, noise_std):\n",
    "    # Generate random x values\n",
    "    max_x = 1\n",
    "    x_values = np.sort(max_x*np.random.random(num_points))\n",
    "    # Generate corresponding y values for polynomial with added noise\n",
    "    y_values =2*x_values-10*x_values**5+15*x_values**10 + np.random.normal(0, noise_std, num_points)\n",
    "    #x_values = np.sort(np.random.uniform(0, 2 * np.pi, num_points))\n",
    "    # generate damped oscillation \n",
    "    #y_values = np.sin(4*x_values)*np.exp(-x_values/2.0) + np.random.normal(0, noise_std, num_points)\n",
    "    return x_values, y_values\n",
    "\n",
    "num_points = 100\n",
    "\n",
    "# You can adjust the noise level as needed\n",
    "x_train, y_train = generate_data(num_points, noise_std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated data for polynomial\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    x_train, y_train,\n",
    "    label='Generated Data with Noise', color='blue', alpha=0.5)\n",
    "\n",
    "plt.plot(\n",
    "    x_train, 2*x_train-10*x_train**5+15*x_train**10,\n",
    "    label='True Polynomial', color='red', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Generated Polynomial Data with Noise')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "YdtRv-OlfGJt",
    "outputId": "d5de9c15-0b22-4dad-e3fd-127cfe615e8c"
   },
   "outputs": [],
   "source": [
    "# Plot the generated data for damped oscillations\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    x_train, y_train,\n",
    "    label='Generated Data with Noise', color='blue', alpha=0.5)\n",
    "plt.plot(\n",
    "    x_train, np.sin(4*x_train)*np.exp(-x_train/2.0),\n",
    "    label='True Curve', color='red', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Generated Data with Noise')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhWveHAqUyB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$\\mathbf{\\text{Step 2: Model Definition}}$\n",
    "\n",
    "We use feedforward neural (also named as a multi-layer perceptron) network to approximate the function  $f(x)$\n",
    "\n",
    "\n",
    "Given an input $x \\in D \\subset R^n$, a feedforward neural network transforms it to an output $NN_\\theta(x)\\in R^m$, through a layer of units (neurons) which compose of either affine-linear maps between units (in successive layers) or scalar non-linear activation functions within units, resulting in the representation,\n",
    "\n",
    "$$NN_{\\theta}(x) = C_K \\circ A \\circ C_{K-1}\\ldots \\ldots \\ldots \\circ A\\circ C_2 \\circ A\\circ C_1(x).$$\n",
    "\n",
    "Here, $\\circ$ refers to the composition of functions and $A$ is a scalar (non-linear) activation function. For any $1 \\leq k \\leq K$, we define\n",
    "\n",
    "$$\n",
    "C_k z_k = W_k z_k + b_k, \\quad {\\rm for} ~ W_k \\in R^{d_{k+1} \\times d_k}, z_k \\in R^{d_k}, b_k \\in R^{d_{k+1}}.\n",
    "$$\n",
    "\n",
    "We also denote,\n",
    "$$\n",
    "\\theta = \\{W_k, b_k\\}, \\theta_W = \\{ W_k \\}\\quad \\forall~ 1 \\leq k \\leq K,\n",
    "$$\n",
    "to be the complete set of (tunable) weights for our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN1gKeqkqUyC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # 1 input feature, 10 hidden units\n",
    "        self.activation = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(10, 1)  # 10 hidden units, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYXmofxIqUyD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$\\mathbf{\\text{Step 3: Model Training}}$\n",
    "\n",
    "The neural network $u_{\\theta}$ depends on the tuning parameter $\\theta \\in \\Theta$ of weights and biases. Within the standard paradigm of deep learning, one trains the network by finding tuning parameters $\\theta$ such that a suitable loss function $J(\\theta)$ is minimized.\n",
    "\n",
    "$${\\rm Find}~\\theta^{\\ast} \\in \\Theta:\\quad \\theta^{\\ast} = {\\rm arg}\\min\\limits_{\\theta \\in \\Theta} \\left( J(\\theta)\\right).$$\n",
    "\n",
    "The loss function, for instance, can be chosen as the mean square error between the neural network and the underlying target:\n",
    "\n",
    "$$ J(\\theta) = \\sum_{i}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$$\n",
    "\n",
    "The optimization process is realized with the gradient descent (or more precisely with variants of the gradient descent such as Adam or SGD).\n",
    "\n",
    "1. Compute the loss function over the batch $j$:\n",
    "$J_S(\\theta)=\\sum_{x_i \\in {S}_j}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$\n",
    "\n",
    "2. Compute the gradient of $J_S(\\theta)$ with respect to the network parameters:  $\\nabla_\\theta J_S(\\theta)$\n",
    "\n",
    "3. Update the parameters according to the chosen optimizer, for instance for minibatch stochastic gradient descent $\\theta_{k+1} = \\theta_{k} - \\eta \\nabla_\\theta J_S(\\theta_{k}) $ with $k=1,...,(n_{epoch} n_{batch})$ and $\\eta$ being the learning rate (argument $lr$ in the optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxgATTQtqUyD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **ADAM Optimizer**\n",
    "\n",
    "Adaptive Momement Estimation: adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "#### **LBFGS Optimizer (Generic Idea)**\n",
    "\n",
    "Let us consider a generic function $f: x\\mapsto f(x)$, $x\\in R^d$ and $f(x)\\in R$. The taylor approximation of $\\nabla f$ in a neighbourhood of $x_k$ is\n",
    "\n",
    "$$\\nabla f(x_k + \\delta x) = \\nabla f(x_k) + H_k\\delta x$$\n",
    "\n",
    "with $H_k$ being the Hessian of $f$ at $x_k$.\n",
    "We would like to find the vector $ \\delta x$ such that\n",
    "\n",
    "$$\\nabla f(x_k + \\delta x) = 0$$\n",
    "\n",
    "This leads to\n",
    "\n",
    "$$x_{k+1} = x_k - H_k^{-1}\\nabla f \\quad \\text{(Newton Method)}$$\n",
    "\n",
    "The idea of Quasi-Newton methods (LBFGS belong to this class) is to approximate $H$ with a positive definite matrix $B$, which is easier to compute:\n",
    "\n",
    "$$x_{k+1} = x_k   - B_{k}^{-1}\\nabla f \\quad \\text{(Quasi - Newton Method)}$$\n",
    "\n",
    "The matrix B has to satisfy the following conditions:\n",
    "- Positive definiteness\n",
    "- $B_k$ and $B_{k+1}$ \"sufficiently close\"\n",
    "- Secant equation  $$ B_{k+1}[x_{k+1} - x_k] = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$$\n",
    "\n",
    "A constraint minimization problem can be set to find the update formula for $B_k$\n",
    "(check https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3sOouTUiYm4"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name, model):\n",
    "  if optimizer_name == \"ADAM\":\n",
    "      return optim.Adam(model.parameters())\n",
    "  elif optimizer_name == \"LBFGS\":\n",
    "      return optim.LBFGS(model.parameters())\n",
    "  raise ValueError('Chosen optimizer is unavailable.')\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, inputs, targets):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return closure().item()\n",
    "\n",
    "\n",
    "def evaluate(model, x_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_test.view(-1, 1)).numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYxibsHSia8s"
   },
   "source": [
    "#### Instantiate your model, define your loss function + optimizer, and perform the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjFZwW6m-ajq",
    "outputId": "279a41a0-02d1-44a0-9293-2ef84d35d88c"
   },
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = get_optimizer('ADAM', model)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert data to PyTorch tensors\n",
    "    inputs = torch.tensor(\n",
    "        x_train, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
    "    targets = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Training\n",
    "    loss = train(model, optimizer, criterion, inputs, targets)\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-4NqJLcg0Ok"
   },
   "source": [
    "$\\mathbf{\\text{Step 4: Plotting Predictions}}$\n",
    "\n",
    "\n",
    "Final step, evaluate your predictions at some input locations and do some plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1., 100)\n",
    "y_pred = evaluate(model, x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Original Noisy Training Data', color='orange', alpha=1)\n",
    "plt.plot(x_test,2*x_test-10*x_test**5+15*x_test**10, label='Original True Polynomial', color='blue', alpha=1.0)\n",
    "plt.plot(x_test, y_pred, label='Learned Curve', color='red', alpha=1.0)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting parameters to play with:\n",
    "\n",
    "Size of training data and noise \n",
    "\n",
    "number of learning steps (epochs) and learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare training, testing and predicted data for damped oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1.2*2*np.pi, 100)\n",
    "y_pred = evaluate(model, x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Original Noisy Training Data', color='orange', alpha=1)\n",
    "plt.plot(x_test,np.sin(4*x_test)*np.exp(-x_test/2.0), label='Original Damped Oscillation', color='blue')\n",
    "plt.plot(x_test, y_pred, label='Learned Curve', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need an animation of the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
